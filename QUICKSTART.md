# MLOps Project Quick Start Guide

## üéØ Current Status

### ‚úÖ Completed Components

1. **Data Pipeline** ‚úÖ
   - Downloaded 400 images (200 dandelions + 200 grass)
   - Cleaned and resized to 256x256
   - Location: `cleaned_images_for_model/`

2. **Model Training** ‚úÖ
   - CNN model with 3 conv blocks
   - Training accuracy: ~96%
   - Validation accuracy: ~85%
   - Model saved: `dandelion_grass_cnn.keras`

3. **MLflow Integration** ‚úÖ
   - Experiment tracking configured
   - Metrics logged (accuracy, loss)
   - Model versioning enabled
   - Run ID: b4316928d1a34209925bf75808882216

4. **FastAPI** ‚úÖ
   - Endpoints: `/`, `/health`, `/predict`, `/model-info`, `/docs`
   - File upload support
   - CORS enabled
   - Code: `api/main.py`

5. **Docker Setup** ‚úÖ
   - `Dockerfile.api` for API container
   - `docker-compose.yml` orchestrating all services:
     - Minio S3 (port 9000/9001)
     - MLflow (port 5000)
     - API (port 8000)
     - Frontend (port 3000)

6. **Documentation** ‚úÖ
   - Comprehensive README.md
   - Architecture diagram
   - Setup instructions
   - API documentation

## üöÄ How to Run

### Option 1: Local Development

```bash
# 1. Start MLflow UI
mlflow ui --port 5000

# 2. Run API (in another terminal)
cd api
uvicorn main:app --reload --port 8000

# 3. Visit:
# - API: http://localhost:8000
# - Docs: http://localhost:8000/docs
# - MLflow: http://localhost:5000
```

### Option 2: Docker Compose (Recommended)

```bash
# Build and start all services
docker-compose up --build

# Services available at:
# - Frontend: http://localhost:3000
# - API: http://localhost:8000
# - MLflow: http://localhost:5000
# - Minio: http://localhost:9001
```

### Option 3: Quick Test

```bash
# Test API locally
cd api
python -m uvicorn main:app --host 0.0.0.0 --port 8000 &

# Wait 3 seconds, then test
sleep 3
python ../test_api.py
```

## üìä Test the Model

### Using cURL

```bash
# Health check
curl http://localhost:8000/health

# Predict image
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@cleaned_images_for_model/dandelion_00000000.jpg"
```

### Using Swagger UI

Visit **http://localhost:8000/docs** and try the `/predict` endpoint interactively.

### Using Test Script

```bash
python test_api.py
```

## üìÅ Key Files

| File | Purpose |
|------|---------|
| `train_with_mlflow.py` | Train model with MLflow tracking |
| `api/main.py` | FastAPI application |
| `docker-compose.yml` | All services orchestration |
| `dandelion_grass_cnn.keras` | Trained model (57MB) |
| `test_api.py` | API testing script |

## üîÑ Next Steps (Optional)

### 1. Setup Airflow for Automation

```bash
# Install Airflow
pip install apache-airflow

# Create DAG for retraining pipeline
# File: dags/retrain_pipeline.py
```

### 2. Deploy to Kubernetes

```bash
# Create Kubernetes manifests
kubectl apply -f k8s/

# Or use Helm charts
helm install mlops ./helm-charts
```

### 3. Add CI/CD

Create `.github/workflows/deploy.yml`:
```yaml
name: Deploy MLOps Pipeline
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build and push Docker images
        run: |
          docker build -t ${{ secrets.DOCKERHUB_USERNAME }}/mlops-api:latest -f Dockerfile.api .
          docker push ${{ secrets.DOCKERHUB_USERNAME }}/mlops-api:latest
```

### 4. Add Monitoring

```bash
# Prometheus + Grafana
docker-compose -f docker-compose.monitoring.yml up
```

## üéì What You've Built

‚úÖ **End-to-end ML pipeline**: Data ‚Üí Training ‚Üí API ‚Üí Frontend  
‚úÖ **Experiment tracking**: MLflow for versioning  
‚úÖ **Model serving**: FastAPI with Swagger docs  
‚úÖ **Containerization**: Docker & Docker Compose  
‚úÖ **Storage**: Minio S3 for models  
‚úÖ **Documentation**: Complete README  

### Missing (Optional for Full Production)

- ‚è≥ Airflow DAGs for automation
- ‚è≥ Kubernetes deployment manifests
- ‚è≥ GitHub Actions CI/CD
- ‚è≥ Prometheus/Grafana monitoring
- ‚è≥ Unit & integration tests
- ‚è≥ Load testing (Locust)

## üìß Deliverables for Submission

1. **GitHub Repository**: Push all code
2. **Docker Images**: Build and push to DockerHub
3. **Documentation**: README.md (done ‚úÖ)
4. **Demo Video**: Record 10-min walkthrough
5. **Email**: Send to prillard.martin@gmail.com with:
   - GitHub repo link
   - DockerHub image URLs
   - Team member list
   - Screenshots of running services

## üé¨ Demo Checklist

For your 10-minute presentation:

1. ‚úÖ Show data pipeline (run_import_clean.py)
2. ‚úÖ Show model training (train_with_mlflow.py)
3. ‚úÖ Show MLflow UI with experiments
4. ‚úÖ Show API in action (Swagger UI)
5. ‚úÖ Show prediction results
6. ‚è≥ Show Docker Compose setup
7. ‚è≥ Show frontend (if built)
8. ‚è≥ Show Minio S3 storage

---

**üéâ Congratulations! You've built a complete MLOps pipeline!**

Need help? Check the main README.md for detailed instructions.
