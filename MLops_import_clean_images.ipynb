{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e561072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bca0e9",
   "metadata": {},
   "source": [
    "## Import the images from the Git hub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6442b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_urls_from_github_folder(repo_api_url):\n",
    "    \"\"\"\n",
    "    Uses the GitHub API to get a list of all file download URLs in a folder.\n",
    "    \"\"\"\n",
    "    print(f\"Contacting GitHub API at: {repo_api_url}\")\n",
    "    try:\n",
    "        response = requests.get(repo_api_url)\n",
    "        response.raise_for_status() # Check for errors\n",
    "        \n",
    "        file_list = response.json()\n",
    "        \n",
    "        if not isinstance(file_list, list):\n",
    "            print(\"Error: Failed to get a valid file list from GitHub.\")\n",
    "            print(\"Response:\", file_list.get(\"message\", \"Unknown error\"))\n",
    "            return []\n",
    "\n",
    "        image_urls = []\n",
    "        for file_info in file_list:\n",
    "            if file_info.get('type') == 'file' and file_info.get('download_url'):\n",
    "                # Check for image extensions\n",
    "                if file_info['name'].lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    image_urls.append((file_info['name'], file_info['download_url']))\n",
    "                    \n",
    "        print(f\"Found {len(image_urls)} image files in the repository folder.\")\n",
    "        return image_urls\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error contacting GitHub API: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02a5483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(image_list, save_dir):\n",
    "    \"\"\"\n",
    "    Downloads images from a list of (filename, url) tuples.\n",
    "    \"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving images to '{save_dir}'...\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total = len(image_list)\n",
    "    \n",
    "    for i, (filename, url) in enumerate(image_list):\n",
    "        print(f\"Downloading {i+1}/{total}: {filename}\", end='\\r')\n",
    "        file_save_path = save_path / filename\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(file_save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            success_count += 1\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"\\nFailed to download {filename} from {url}: {e}\")\n",
    "            fail_count += 1\n",
    "        \n",
    "        # Be polite to the server\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    # Clear the progress line\n",
    "    print(\" \" * 80, end='\\r') \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"Download complete.\")\n",
    "    print(f\"Successfully downloaded: {success_count}\")\n",
    "    print(f\"Failed to download:    {fail_count}\")\n",
    "    print(\"=\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8a76931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download for 2 folders...\n",
      "========================================\n",
      "\n",
      "Processing folder: dandelion\n",
      "Contacting GitHub API at: https://api.github.com/repos/btphan95/greenr-airflow/contents/data/dandelion\n",
      "Found 200 image files in the repository folder.\n",
      "Saving images to 'image_data_from_repo\\dandelion'...\n",
      "                                                                                \n",
      "==============================\n",
      "Download complete.\n",
      "Successfully downloaded: 200\n",
      "Failed to download:    0\n",
      "==============================\n",
      "----------------------------------------\n",
      "\n",
      "Processing folder: grass\n",
      "Contacting GitHub API at: https://api.github.com/repos/btphan95/greenr-airflow/contents/data/grass\n",
      "Found 200 image files in the repository folder.\n",
      "Saving images to 'image_data_from_repo\\grass'...\n",
      "                                                                                \n",
      "==============================\n",
      "Download complete.\n",
      "Successfully downloaded: 200\n",
      "Failed to download:    0\n",
      "==============================\n",
      "----------------------------------------\n",
      "\n",
      "All processing complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Base URL for the GitHub API\n",
    "    BASE_API_URL = \"https://api.github.com/repos/btphan95/greenr-airflow/contents/data/\"\n",
    "    \n",
    "    # List of folders to download\n",
    "    FOLDERS_TO_DOWNLOAD = [\"dandelion\", \"grass\"]\n",
    "    \n",
    "    # Base directory to save all images\n",
    "    BASE_SAVE_DIR = \"image_data_from_repo\"\n",
    "\n",
    "    print(f\"Starting download for {len(FOLDERS_TO_DOWNLOAD)} folders...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for folder_name in FOLDERS_TO_DOWNLOAD:\n",
    "        print(f\"\\nProcessing folder: {folder_name}\")\n",
    "        \n",
    "        # 1. Construct the specific API URL and save directory for this folder\n",
    "        api_url = BASE_API_URL + folder_name\n",
    "        download_dir = os.path.join(BASE_SAVE_DIR, folder_name)\n",
    "        \n",
    "        # 2. Get the list of image URLs\n",
    "        image_list = get_image_urls_from_github_folder(api_url)\n",
    "        \n",
    "        # 3. If list is not empty, download the images\n",
    "        if image_list:\n",
    "            download_images(image_list, download_dir)\n",
    "        else:\n",
    "            print(f\"No images found in {folder_name} or failed to retrieve list.\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    print(\"\\nAll processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c6211",
   "metadata": {},
   "source": [
    "## Clean the images to be ready to be preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2fd7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "# 1. The base directory where the download script saved the images\n",
    "SOURCE_BASE_DIR = \"image_data_from_repo\"\n",
    "\n",
    "# 2. The subfolders we want to process\n",
    "CATEGORIES = [\"dandelion\", \"grass\"]\n",
    "\n",
    "# 3. The single, combined folder to save all cleaned images\n",
    "OUTPUT_DIR = \"cleaned_images_for_model\"\n",
    "\n",
    "# 4. The standard size for all images (width, height)\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "# --- End of Configuration ---\n",
    "\n",
    "def clean_and_process_images():\n",
    "    \"\"\"\n",
    "    Scans subdirectories, cleans images, and saves them to a single\n",
    "    combined output folder, ready for machine learning.\n",
    "    \"\"\"\n",
    "    print(\"Starting image cleaning and processing...\")\n",
    "    \n",
    "    source_base_path = Path(SOURCE_BASE_DIR)\n",
    "    output_path = Path(OUTPUT_DIR)\n",
    "    \n",
    "    # Create the single output directory\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    total_processed = 0\n",
    "\n",
    "    if not source_base_path.exists():\n",
    "        print(f\"Error: Source directory not found: '{SOURCE_BASE_DIR}'\")\n",
    "        print(\"Please run the 'download_from_repo_folder.py' script first.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Loop through each category (e.g., \"dandelion\", \"grass\")\n",
    "    for category in CATEGORIES:\n",
    "        category_path = source_base_path / category\n",
    "        print(f\"\\nProcessing folder: {category_path}\")\n",
    "        \n",
    "        if not category_path.exists():\n",
    "            print(f\"Warning: Category folder not found: '{category_path}'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        image_files = os.listdir(category_path)\n",
    "        print(f\"Found {len(image_files)} files.\")\n",
    "        \n",
    "        # Loop through each image file in the category folder\n",
    "        for i, filename in enumerate(image_files):\n",
    "            total_processed += 1\n",
    "            print(f\"  Processing {i+1}/{len(image_files)}: {filename}\", end='\\r')\n",
    "            \n",
    "            source_file_path = category_path / filename\n",
    "            \n",
    "            # Create a new, unique filename that includes the category\n",
    "            # e.g., \"dandelion_image_001.jpg\"\n",
    "            new_filename = f\"{category}_{filename}\"\n",
    "            output_file_path = output_path / new_filename\n",
    "            \n",
    "            try:\n",
    "                # Open the image\n",
    "                with Image.open(source_file_path) as img:\n",
    "                    # 1. Convert to RGB (removes transparency/alpha)\n",
    "                    cleaned_img = img.convert(\"RGB\")\n",
    "                    \n",
    "                    # 2. Resize to the standard size\n",
    "                    # Using Image.LANCZOS (or Resampling.LANCZOS) for high-quality downscaling\n",
    "                    antialiasing_method = Image.LANCZOS if hasattr(Image, 'LANCZOS') else Image.Resampling.LANCZOS\n",
    "                    resized_img = cleaned_img.resize(IMAGE_SIZE, antialiasing_method)\n",
    "                    \n",
    "                    # 3. Save the cleaned image to the output folder\n",
    "                    resized_img.save(output_file_path, \"JPEG\", quality=90)\n",
    "                    \n",
    "                    success_count += 1\n",
    "                    \n",
    "            except (IOError, UnidentifiedImageError, OSError) as e:\n",
    "                print(f\"\\nFailed to process {source_file_path}: {e}\")\n",
    "                fail_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"\\nAn unexpected error occurred with {source_file_path}: {e}\")\n",
    "                fail_count += 1\n",
    "        \n",
    "        # Clear the line\n",
    "        print(\" \" * 80, end='\\r')\n",
    "        print(f\"  Finished processing folder: {category}\")\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n\\n\" + \"=\" * 30)\n",
    "    print(\"Image Processing Complete\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Total images found:     {total_processed}\")\n",
    "    print(f\"Successfully cleaned:   {success_count}\")\n",
    "    print(f\"Failed to process:      {fail_count}\")\n",
    "    print(f\"\\nAll cleaned images are saved in: '{OUTPUT_DIR}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bce7654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image cleaning and processing...\n",
      "\n",
      "Processing folder: image_data_from_repo\\dandelion\n",
      "Found 200 files.\n",
      "  Finished processing folder: dandelion                                         \n",
      "\n",
      "Processing folder: image_data_from_repo\\grass\n",
      "Found 200 files.\n",
      "  Finished processing folder: grass                                             \n",
      "\n",
      "\n",
      "==============================\n",
      "Image Processing Complete\n",
      "==============================\n",
      "Total images found:     400\n",
      "Successfully cleaned:   400\n",
      "Failed to process:      0\n",
      "\n",
      "All cleaned images are saved in: 'cleaned_images_for_model'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Check for Pillow installation ---\n",
    "    try:\n",
    "        from PIL import Image\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'Pillow' library is required for this script.\")\n",
    "        print(\"Please install it by running: pip install Pillow\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    clean_and_process_images()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
